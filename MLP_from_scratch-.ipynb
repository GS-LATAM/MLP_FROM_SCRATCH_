{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Ideia é criar uma rede neural MLP com uma e duas hidden layers sem a utilização de nenhuma biblioteca dedicada (ex. sklearn) para as funções core da rede e aplicá-la, sem alteração de arquitetura, sobre dois datasets.\n",
    "- Datasets utilizados:\n",
    "    - wine.data1 (classificação, https://archive.ics.uci.edu/ml/datasets/Wine)\n",
    "    - default_features_1059_tracks.txt2 (regressão / aproximação, https://archive.ics.uci.edu/ml/datasets/Geographical+Original+of+Music)\n",
    "- Métricas: \n",
    "    - Acurácia para problema de classificação\n",
    "    - RMSE para problema de regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importa bibliotecas utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Classe da rede MLP com seus parâmetros e funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "\n",
    "    #Parâmetros da rede \n",
    "    def __init__(self, eta = 0.1, epoch =10000, epsilon=0.01, alfa=0, i=13, j=3, k=3, nlayer=1, job='c'):\n",
    "        self.eta = eta # taxa de aprendizagem\n",
    "        self.epoch = epoch # número máximo de épocas\n",
    "        self.epsilon = epsilon # erro médio admissível\n",
    "        self.alfa=alfa # fator do termo momentum\n",
    "        self.i=i # número de neurônios na camada de entrada. Deve ter a dimensionalidade das features\n",
    "        self.j=j # número de neurônios na camada j\n",
    "        self.k=k # número de neurônios na camada k. Deve ter \n",
    "        self.nlayer= nlayer # número de camadas intermediárias\n",
    "        self.job = job # 'c' para caso de classificação e 'r' para caso de regressão\n",
    "        if self.job == 'c': # adaptação para caso geral e correto parse do cálculo de erro\n",
    "            self.l=nlayer+1 # número de neurônios na camada l\n",
    "        else:\n",
    "            self.l=nlayer # número de neurônios na camada l\n",
    "\n",
    "    #Inicializa os pesos (matriz Wji) e bias (vetor teta_j) da rede. \n",
    "    #j,i representam os números de neurônios da camada de saída e entrada    \n",
    "    def inicialize(self): \n",
    "\n",
    "        self.w_ji=np.random.uniform(-1,1, [self.j,self.i+1]) # Inicialização com parâmetros randômicos\n",
    "        self.w_kj=np.random.uniform(-1,1, [self.k,self.j+1]) # Inicialização com parâmetros randômicos\n",
    "\n",
    "        if (self.nlayer>1):\n",
    "          self.w_lk=np.random.uniform(-1,1, [self.l,self.k+1]) # Inicialização com parâmetros randômicos\n",
    "\n",
    "        return self\n",
    "\n",
    "    #Função de ativação utilizada - sigmoide\n",
    "    def sigm(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    #Derivada da função sigmoide\n",
    "    def d_sigm(self, x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    #Cálculo da saída da camada escondida      \n",
    "    def feed_net(self, y_i):\n",
    "        self.y_i=y_i #entrada da camada i\n",
    "        self.y_j=self.sigm(np.dot(self.w_ji,np.concatenate((y_i,[1])))) #entrada da camada j, termo adicionado devido ao bias\n",
    "        self.y_k=self.sigm(np.dot(self.w_kj,np.concatenate((self.y_j,[1])))) #entrada da camada K, termo adicionado devido ao bias\n",
    "\n",
    "        if (self.nlayer>1):\n",
    "          self.y_l=self.sigm(np.dot(self.w_lk,np.concatenate((self.y_k,[1])))) #entrada da camada L, termo adicionado devido ao bias\n",
    "        \n",
    "        return self\n",
    "\n",
    "    #Cálculo dos gradientes  para backpropagation\n",
    "    def back_propagation(self, error):\n",
    "        \n",
    "        if (self.nlayer>1):\n",
    "          self.delta_l=error*self.d_sigm(self.y_l) #Gradiente local do erro na camada de saída\n",
    "          self.delta_k=np.dot(self.w_lk.T,self.delta_l)*self.d_sigm(np.concatenate((self.y_k,[1])))#Gradiente local do erro na camada escondida\n",
    "          self.delta_k=self.delta_k[0:self.k] # O último elemento representava apenas a relação do erro do bias, mas o bias não se interliga com a camada anterior.\n",
    "          \n",
    "        else:\n",
    "          self.delta_k=error*self.d_sigm(self.y_k) #Gradiente local do erro na camada de saída\n",
    "        \n",
    "        self.delta_j=np.dot(self.w_kj.T,self.delta_k)*self.d_sigm(np.concatenate((self.y_j,[1]))) #Gradiente local do erro na camada escondida\n",
    "        self.delta_j=self.delta_j[0:self.j] # O último elemento representava apenas a relação do erro do bias, mas o bias não se interliga com a camada anterior.  \n",
    "\n",
    "        return self\n",
    "\n",
    "    #Fit para atualização dos pesos.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        emed=1.01 # Inicializa erro aceitável com valor maior que o range usual para garantir primeira iteração\n",
    "        count=0 # Inicializa contador\n",
    "        self.inicialize()\n",
    "        #Inicia delta pesos com valores zerados\n",
    "        deltaw_lk=0 \n",
    "        deltaw_kj=0\n",
    "        deltaw_ji=0\n",
    "\n",
    "        \n",
    "        while (emed > self.epsilon and count < self.epoch):\n",
    "         \n",
    "         index = np.arange(X.shape[0])\n",
    "         np.random.shuffle(index)\n",
    "         X = X[index]\n",
    "         y = y[index]\n",
    "\n",
    "         emed=0 #Zera o erro aceitável para posterior atualização\n",
    "         \n",
    "         for n in range (np.size(X, 0)):\n",
    "\n",
    "           y_i=X[n,:]\n",
    "           self.feed_net(y_i)      \n",
    "           # Calcula erros \n",
    "           if (self.nlayer>1):\n",
    "             error=y[n,:]-self.y_l\n",
    "           else:\n",
    "             error=y[n,:]-self.y_k\n",
    "           #Atualiza erro médio e calcula gradientes para backpropagation            \n",
    "           emed += np.sum(np.square(error))\n",
    "           self.back_propagation(error) \n",
    "            #Atualiza os pesos \n",
    "           if (self.nlayer>1):\n",
    "             self.w_lk=self.w_lk+self.eta*self.delta_l[:, np.newaxis]*np.concatenate((self.y_k,[1])) + self.alfa*deltaw_lk\n",
    "          \n",
    "           self.w_kj=self.w_kj+self.eta*self.delta_k[:, np.newaxis]*np.concatenate((self.y_j,[1])) + self.alfa*deltaw_kj\n",
    "           self.w_ji=self.w_ji+self.eta*self.delta_j[:, np.newaxis]*np.concatenate((self.y_i,[1]))+ self.alfa*deltaw_ji\n",
    "\n",
    "           if (self.nlayer>1):\n",
    "             deltaw_lk=self.eta*self.delta_l[:, np.newaxis]*np.concatenate((self.y_k,[1])) + self.alfa*deltaw_lk\n",
    "           \n",
    "           deltaw_kj=self.eta*self.delta_k[:, np.newaxis]*np.concatenate((self.y_j,[1])) + self.alfa*deltaw_kj\n",
    "           deltaw_ji=-self.eta*self.delta_j[:, np.newaxis]*np.concatenate((self.y_i,[1])) + self.alfa*deltaw_ji\n",
    "           \n",
    "           self.feed_net(y_i)\n",
    "          \n",
    "         emed /= np.size(X, 0)\n",
    "         self.emed=emed\n",
    "         count += 1\n",
    "        # Contador para posterior tabulação\n",
    "        self.counter = count\n",
    "\n",
    "        return self\n",
    "\n",
    "    #Define função para realização de predições\n",
    "    def predict(self, X):\n",
    "        ypred=[]\n",
    "        for n in range (np.size(X, 0)):\n",
    "          y_i=X[n,:]\n",
    "          self.feed_net(y_i)\n",
    "\n",
    "          if (self.nlayer>1):\n",
    "            ypred.append(self.y_l)\n",
    "          else: \n",
    "            ypred.append(self.y_k)\n",
    "\n",
    "        return np.array(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lê url da tarefa e retorna dataframe\n",
    "df_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                      sep=\",\",\n",
    "                      names = ['Origin','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols', \n",
    "                               'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue',\n",
    "                               'OD280/OD315 of diluted wines', 'Proline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-processamento\n",
    "#Define features e target\n",
    "X = df_data.drop('Origin', axis=1)\n",
    "y = df_data['Origin']\n",
    "#Aplica one hot encoder sobre target dando dimensionalidade correta das classes com geração de vetores linearmente independentes\n",
    "enc = OneHotEncoder()\n",
    "y = y.values.reshape(-1,1)\n",
    "y_ready = enc.fit_transform(y).toarray()\n",
    "#Aplica std.Scaler e minmax scalers. Assim eliminamos a variância entre os domínios das features e, com a mesma escala\n",
    "    #evitamos desbalanceamento no cálculo das fnets, já que evita que a derivada da função sigmoidal tenda a zero, o que implicaria\n",
    "    #em reduzida atualização dos pesos, fazendo-os estagnar e não proporcionando aprendizado.\n",
    "scaler = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_ready = minmax.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.993\n",
      "accuracy_test_set = 0.889\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.81\n",
      "accuracy_test_set = 0.778\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.979\n",
      "accuracy_test_set = 0.889\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.866\n",
      "accuracy_test_set = 0.722\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  223\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.972\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.937\n",
      "accuracy_test_set = 0.944\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  435\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.972\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.859\n",
      "accuracy_test_set = 0.806\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.993\n",
      "accuracy_test_set = 0.972\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.859\n",
      "accuracy_test_set = 0.75\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.986\n",
      "accuracy_test_set = 0.944\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.606\n",
      "accuracy_test_set = 0.556\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  115\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.917\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.852\n",
      "accuracy_test_set = 0.861\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  241\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.917\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.937\n",
      "accuracy_test_set = 0.917\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.978\n",
      "accuracy_test_set = 0.966\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.82\n",
      "accuracy_test_set = 0.764\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.944\n",
      "accuracy_test_set = 0.921\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.843\n",
      "accuracy_test_set = 0.82\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  336\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.966\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.944\n",
      "accuracy_test_set = 0.933\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.978\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.82\n",
      "accuracy_test_set = 0.831\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.978\n",
      "accuracy_test_set = 0.978\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.596\n",
      "accuracy_test_set = 0.596\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.989\n",
      "accuracy_test_set = 0.966\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  50\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "accuracy_train_set = 0.472\n",
      "accuracy_test_set = 0.517\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.2\n",
      "epochs_used =  171\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.978\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.719\n",
      "accuracy_test_set = 0.697\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.2\n",
      "epochs_used =  376\n",
      "accuracy_train_set = 1.0\n",
      "accuracy_test_set = 0.978\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "available_epochs =  500\n",
      "learning_rate =  0.1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "accuracy_train_set = 0.697\n",
      "accuracy_test_set = 0.584\n",
      "+_+_+_+_+_+_+\n"
     ]
    }
   ],
   "source": [
    "#Define os valores que cada um dos parâmetros assumirá durante as iterações, que terão todas as combinações destes valores\n",
    "test_size_iter = [0.2, 0.5]\n",
    "hidden_layer_iter = [1, 2]\n",
    "epoch_amount_iter = [50, 500]\n",
    "learning_rate_iter = [0.2, 0.1]\n",
    "momentum_term_iter = [0.2, 0.5]\n",
    "\n",
    "\n",
    "#Listas para posterior tabulação\n",
    "out_train_size = []\n",
    "out_hidden_layer = []\n",
    "out_epoch = []\n",
    "out_learning_rate = []\n",
    "out_momentum = []\n",
    "out_train_acc = []\n",
    "out_test_acc = []\n",
    "\n",
    "#Gera a combinação de parâmetros para alimentar as iterações sobre a rede\n",
    "for test_size in test_size_iter:\n",
    "    #Divide os arrays de treino, testes, features e targets com random state fixo, para posterior incremento.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ready, y_ready, test_size=test_size, random_state=8)\n",
    "    for hidden_layer in hidden_layer_iter:\n",
    "        for epoch_amount in epoch_amount_iter:\n",
    "            for learning_rate in learning_rate_iter:\n",
    "                for momentum_term in momentum_term_iter:\n",
    "                    train_size = 1-test_size #Calcula proporção de treino para posterior tabulação\n",
    "                    print ('train_size = ', train_size) #Imprime parâmetros a cada iteração\n",
    "                    print ('hidden_layers = ', hidden_layer) #Imprime parâmetros a cada iteração\n",
    "                    print ('available_epochs = ', epoch_amount) #Imprime parâmetros a cada iteração\n",
    "                    print ('learning_rate = ', learning_rate) #Imprime parâmetros a cada iteração\n",
    "                    print ('momentum_term = ', momentum_term) #Imprime parâmetros a cada iteração\n",
    "                    #Gera a rede com os parâmetros acima descritos\n",
    "                    nn = NeuralNetwork(eta=learning_rate,\n",
    "                                       epoch=epoch_amount, \n",
    "                                       epsilon=0.005, \n",
    "                                       alfa=momentum_term, \n",
    "                                       i=X.shape[1], j=10, k=3, \n",
    "                                       nlayer=hidden_layer, \n",
    "                                       job='c')\n",
    "                    fitted = nn.fit(X_train,y_train) #Atualização dos pesos\n",
    "                    y_train_pred = np.around(nn.predict(X_train)) #Predição sobre grupo de treino\n",
    "                    y_test_pred = np.around(nn.predict(X_test)) #Predição sobre grupo de teste\n",
    "                    train_acc = round(accuracy_score(y_train, y_train_pred),3) #Cálculo da acurácia de treino para posterior tabulação\n",
    "                    test_acc = round(accuracy_score(y_test, y_test_pred),3) #Cálculo da acuracia de teste para posterior tabulação\n",
    "                    print ('epochs_used = ', fitted.counter) #Imprime quantidades de ciclos utilizados a cada iteração até que \n",
    "                                                                #o valor de erro aceitável seja atingido ou que o número de ciclos\n",
    "                                                                #disponíveis se esgote\n",
    "                    print ('accuracy_train_set =', train_acc) #Imprime acurácia de treino a cada iteração\n",
    "                    print ('accuracy_test_set =', test_acc) #Imprime acurácia de teste a cada iteração\n",
    "                    #Atualiza listas com valores da iteração para posterior tabulação\n",
    "                    out_train_size.append(train_size) \n",
    "                    out_hidden_layer.append(hidden_layer)\n",
    "                    out_epoch.append(fitted.counter)\n",
    "                    out_learning_rate.append(learning_rate)\n",
    "                    out_momentum.append(momentum_term)\n",
    "                    out_train_acc.append(train_acc)\n",
    "                    out_test_acc.append(test_acc)\n",
    "                    print ('+_+_+_+_+_+_+') #Imprime fim da iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta dataframe tabulado para posterior relatório\n",
    "df_table_class = pd.DataFrame()\n",
    "df_table_class['Percentual Treino'] = out_train_size\n",
    "df_table_class['Camadas Intermediárias'] = out_hidden_layer\n",
    "df_table_class['Ciclos para Treinamento'] = out_epoch\n",
    "df_table_class['Velocidade de Aprendizado'] = out_learning_rate\n",
    "df_table_class['Termo Momentum'] = out_momentum\n",
    "df_table_class['Acurácia Dados Treino'] = out_train_acc\n",
    "df_table_class['Acurácia Dados Teste'] = out_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percentual Treino</th>\n",
       "      <th>Camadas Intermediárias</th>\n",
       "      <th>Ciclos para Treinamento</th>\n",
       "      <th>Velocidade de Aprendizado</th>\n",
       "      <th>Termo Momentum</th>\n",
       "      <th>Acurácia Dados Treino</th>\n",
       "      <th>Acurácia Dados Teste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>376</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>435</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>336</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>241</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.993</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Percentual Treino  Camadas Intermediárias  Ciclos para Treinamento  \\\n",
       "0                 0.5                       2                       50   \n",
       "1                 0.5                       2                      376   \n",
       "2                 0.5                       2                      171   \n",
       "3                 0.5                       1                      500   \n",
       "4                 0.8                       2                       50   \n",
       "5                 0.8                       1                      223   \n",
       "6                 0.8                       1                      435   \n",
       "7                 0.5                       1                       50   \n",
       "8                 0.5                       2                       50   \n",
       "9                 0.5                       1                      336   \n",
       "10                0.8                       2                       50   \n",
       "11                0.8                       1                      500   \n",
       "12                0.5                       1                      500   \n",
       "13                0.5                       1                       50   \n",
       "14                0.8                       2                      115   \n",
       "15                0.8                       2                      241   \n",
       "16                0.8                       2                      500   \n",
       "17                0.8                       1                       50   \n",
       "18                0.8                       1                       50   \n",
       "19                0.8                       2                      500   \n",
       "20                0.5                       1                      500   \n",
       "21                0.5                       1                       50   \n",
       "22                0.8                       1                      500   \n",
       "23                0.8                       1                       50   \n",
       "24                0.5                       1                       50   \n",
       "25                0.8                       2                       50   \n",
       "26                0.8                       1                       50   \n",
       "27                0.5                       2                      500   \n",
       "28                0.5                       2                       50   \n",
       "29                0.5                       2                      500   \n",
       "30                0.8                       2                       50   \n",
       "31                0.5                       2                       50   \n",
       "\n",
       "    Velocidade de Aprendizado  Termo Momentum  Acurácia Dados Treino  \\\n",
       "0                         0.2             0.2                  0.978   \n",
       "1                         0.1             0.2                  1.000   \n",
       "2                         0.2             0.2                  1.000   \n",
       "3                         0.1             0.2                  1.000   \n",
       "4                         0.2             0.2                  0.993   \n",
       "5                         0.2             0.2                  1.000   \n",
       "6                         0.1             0.2                  1.000   \n",
       "7                         0.2             0.2                  0.978   \n",
       "8                         0.1             0.2                  0.989   \n",
       "9                         0.2             0.2                  1.000   \n",
       "10                        0.1             0.2                  0.986   \n",
       "11                        0.2             0.5                  0.937   \n",
       "12                        0.2             0.5                  0.944   \n",
       "13                        0.1             0.2                  0.944   \n",
       "14                        0.2             0.2                  1.000   \n",
       "15                        0.1             0.2                  1.000   \n",
       "16                        0.1             0.5                  0.937   \n",
       "17                        0.2             0.2                  0.993   \n",
       "18                        0.1             0.2                  0.979   \n",
       "19                        0.2             0.5                  0.852   \n",
       "20                        0.1             0.5                  0.820   \n",
       "21                        0.1             0.5                  0.843   \n",
       "22                        0.1             0.5                  0.859   \n",
       "23                        0.2             0.5                  0.810   \n",
       "24                        0.2             0.5                  0.820   \n",
       "25                        0.2             0.5                  0.859   \n",
       "26                        0.1             0.5                  0.866   \n",
       "27                        0.2             0.5                  0.719   \n",
       "28                        0.2             0.5                  0.596   \n",
       "29                        0.1             0.5                  0.697   \n",
       "30                        0.1             0.5                  0.606   \n",
       "31                        0.1             0.5                  0.472   \n",
       "\n",
       "    Acurácia Dados Teste  \n",
       "0                  0.978  \n",
       "1                  0.978  \n",
       "2                  0.978  \n",
       "3                  0.978  \n",
       "4                  0.972  \n",
       "5                  0.972  \n",
       "6                  0.972  \n",
       "7                  0.966  \n",
       "8                  0.966  \n",
       "9                  0.966  \n",
       "10                 0.944  \n",
       "11                 0.944  \n",
       "12                 0.933  \n",
       "13                 0.921  \n",
       "14                 0.917  \n",
       "15                 0.917  \n",
       "16                 0.917  \n",
       "17                 0.889  \n",
       "18                 0.889  \n",
       "19                 0.861  \n",
       "20                 0.831  \n",
       "21                 0.820  \n",
       "22                 0.806  \n",
       "23                 0.778  \n",
       "24                 0.764  \n",
       "25                 0.750  \n",
       "26                 0.722  \n",
       "27                 0.697  \n",
       "28                 0.596  \n",
       "29                 0.584  \n",
       "30                 0.556  \n",
       "31                 0.517  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imprime dataframe com valores de acurácia de dados de teste descendentes\n",
    "df_table_class = df_table_class.sort_values(by='Acurácia Dados Teste', ascending=False, ignore_index=True)\n",
    "df_table_class.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lê a url disponibilizada, baixa o arquivo .zip e o extrai para pasta C:/temp_work\n",
    "import requests, zipfile, io\n",
    "r = requests.get('https://archive.ics.uci.edu/ml/machine-learning-databases/00315/Geographical%20Original%20of%20Music.zip')\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall('C:/temp_work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria dataframe com o arquivo de interesse conforme orientação da tarefa\n",
    "df = pd. read_csv (r'C:/temp_work/Geographical Original of Music/default_features_1059_tracks.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pré-processamento\n",
    "#Define features e target\n",
    "X = df.drop(df.columns[-2:], axis=1)\n",
    "y = df.drop(df.columns[:-2], axis=1)\n",
    "#Aplica std.Scaler e minmax scalers. Assim eliminamos a variância entre os domínios das features e, com a mesma escala\n",
    "    #evitamos desbalanceamento no cálculo das fnets, já evita que a derivada da função sigmoidal tenda a zero, o que implicaria\n",
    "    #em reduzida atualização dos pesos, fazendo-os stagnares e não proporcionando aprendizado.\n",
    "scaler = StandardScaler()\n",
    "minmax = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_ready = minmax.fit_transform(X)\n",
    "#Cria array do target\n",
    "y_ready = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 50.272\n",
      "RMSE_test_set = 50.542\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 50.011\n",
      "RMSE_test_set = 50.285\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.897\n",
      "RMSE_test_set = 50.154\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.633\n",
      "RMSE_test_set = 49.895\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.8\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 50.011\n",
      "RMSE_test_set = 50.285\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 50.134\n",
      "RMSE_test_set = 49.762\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  1\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  50\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  50\n",
      "RMSE_train_set = 50.24\n",
      "RMSE_test_set = 49.892\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  0.2\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.1\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n",
      "train_size =  0.5\n",
      "hidden_layers =  2\n",
      "epochs =  500\n",
      "learning_rate =  1\n",
      "momentum_term =  0.5\n",
      "epochs_used =  500\n",
      "RMSE_train_set = 49.874\n",
      "RMSE_test_set = 49.497\n",
      "+_+_+_+_+_+_+\n"
     ]
    }
   ],
   "source": [
    "#Define os valores que cada um dos parâmetros assumirá durante as iterações, que terão todas as combinações destes valores\n",
    "test_size_iter = [0.2, 0.5]\n",
    "hidden_layer_iter = [1, 2]\n",
    "epoch_amount_iter = [50, 500]\n",
    "learning_rate_iter = [0.2, 1]\n",
    "momentum_term_iter = [0.1, 0.5]\n",
    "\n",
    "\n",
    "#Listas para posterior tabulação\n",
    "out_train_size = []\n",
    "out_hidden_layer = []\n",
    "out_epoch = []\n",
    "out_learning_rate = []\n",
    "out_momentum = []\n",
    "out_train_rmse = []\n",
    "out_test_rmse = []\n",
    "\n",
    "#Gera a combinação de parâmetros para alimentar as iterações sobre a rede\n",
    "for test_size in test_size_iter:\n",
    "    #Divide os arrays de treino, testes, features e targets com random state fixo, para posterior incremento.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ready, y_ready, test_size=test_size, random_state=8)\n",
    "    for hidden_layer in hidden_layer_iter:\n",
    "        for epoch_amount in epoch_amount_iter:\n",
    "            for learning_rate in learning_rate_iter:\n",
    "                for momentum_term in momentum_term_iter:\n",
    "                    train_size = 1-test_size #Calcula proporção de treino para posterior tabulação\n",
    "                    print ('train_size = ', train_size) #Imprime parâmetros a cada iteração\n",
    "                    print ('hidden_layers = ', hidden_layer) #Imprime parâmetros a cada iteração\n",
    "                    print ('epochs = ', epoch_amount) #Imprime parâmetros a cada iteração\n",
    "                    print ('learning_rate = ', learning_rate) #Imprime parâmetros a cada iteração\n",
    "                    print ('momentum_term = ', momentum_term) #Imprime parâmetros a cada iteração\n",
    "                    #Gera a rede com os parâmetros acima descritos\n",
    "                    nn = NeuralNetwork(eta=learning_rate,\n",
    "                                       epoch=epoch_amount, \n",
    "                                       epsilon=0.005, \n",
    "                                       alfa=momentum_term, \n",
    "                                       i=X.shape[1], j=38, k=2, \n",
    "                                       nlayer=hidden_layer, \n",
    "                                       job='r')\n",
    "                    fitted = nn.fit(X_train,y_train) #Atualização dos pesos\n",
    "                    y_train_pred = nn.predict(X_train) #Predição sobre grupo de treino\n",
    "                    y_test_pred = nn.predict(X_test) #Predição sobre grupo de teste\n",
    "                    train_rmse = round(np.sqrt(mean_squared_error(y_train, y_train_pred)),3) #Cálculo de RMSE de treino para posterior tabulação\n",
    "                    test_rmse = round(np.sqrt(mean_squared_error(y_test, y_test_pred)),3) #Cálculo de RMSE de treino para posterior tabulação\n",
    "                    print ('epochs_used = ', fitted.counter) #Imprime quantidades de ciclos utilizados a cada iteração até que \n",
    "                                                                #o valor de erro aceitável seja atingido ou que o número de ciclos\n",
    "                                                                #disponíveis se esgote\n",
    "                    print ('RMSE_train_set =', train_rmse) #Imprime RMSE a cada iteração\n",
    "                    print ('RMSE_test_set =', test_rmse) #Imprime RMSE a cada iteração\n",
    "                    #Atualiza listas com valores da iteração para posterior tabulação\n",
    "                    out_train_size.append(train_size)\n",
    "                    out_hidden_layer.append(hidden_layer)\n",
    "                    out_epoch.append(epoch_amount)\n",
    "                    out_learning_rate.append(learning_rate)\n",
    "                    out_momentum.append(momentum_term)\n",
    "                    out_train_rmse.append(train_rmse)\n",
    "                    out_test_rmse.append(test_rmse)\n",
    "                    print ('+_+_+_+_+_+_+') #Imprime fim da iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta dataframe tabulado para posterior relatório\n",
    "df_table_reg = pd.DataFrame()\n",
    "df_table_reg['Percentual Treino'] = out_train_size\n",
    "df_table_reg['Camadas Intermediárias'] = out_hidden_layer\n",
    "df_table_reg['Ciclos para Treinamento'] = out_epoch\n",
    "df_table_reg['Velocidade de Aprendizado'] = out_learning_rate\n",
    "df_table_reg['Termo Momentum'] = out_momentum\n",
    "df_table_reg['RMSE Dados Treino'] = out_train_rmse\n",
    "df_table_reg ['RMSE Dados Teste'] = out_test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percentual Treino</th>\n",
       "      <th>Camadas Intermediárias</th>\n",
       "      <th>Ciclos para Treinamento</th>\n",
       "      <th>Velocidade de Aprendizado</th>\n",
       "      <th>Termo Momentum</th>\n",
       "      <th>RMSE Dados Treino</th>\n",
       "      <th>RMSE Dados Teste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.272</td>\n",
       "      <td>50.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50.011</td>\n",
       "      <td>50.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.011</td>\n",
       "      <td>50.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.897</td>\n",
       "      <td>50.154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.633</td>\n",
       "      <td>49.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50.240</td>\n",
       "      <td>49.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.134</td>\n",
       "      <td>49.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>49.874</td>\n",
       "      <td>49.497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Percentual Treino  Camadas Intermediárias  Ciclos para Treinamento  \\\n",
       "0                 0.8                       1                       50   \n",
       "1                 0.8                       2                      500   \n",
       "2                 0.8                       1                      500   \n",
       "3                 0.8                       2                       50   \n",
       "4                 0.8                       1                       50   \n",
       "5                 0.8                       2                       50   \n",
       "6                 0.8                       1                       50   \n",
       "7                 0.8                       2                      500   \n",
       "8                 0.8                       2                      500   \n",
       "9                 0.8                       2                      500   \n",
       "10                0.8                       2                       50   \n",
       "11                0.8                       2                       50   \n",
       "12                0.8                       1                      500   \n",
       "13                0.8                       1                      500   \n",
       "14                0.8                       1                      500   \n",
       "15                0.8                       1                       50   \n",
       "16                0.5                       2                       50   \n",
       "17                0.5                       1                      500   \n",
       "18                0.5                       2                       50   \n",
       "19                0.5                       2                      500   \n",
       "20                0.5                       2                      500   \n",
       "21                0.5                       2                      500   \n",
       "22                0.5                       2                       50   \n",
       "23                0.5                       2                       50   \n",
       "24                0.5                       1                       50   \n",
       "25                0.5                       1                      500   \n",
       "26                0.5                       1                      500   \n",
       "27                0.5                       1                      500   \n",
       "28                0.5                       1                       50   \n",
       "29                0.5                       1                       50   \n",
       "30                0.5                       1                       50   \n",
       "31                0.5                       2                      500   \n",
       "\n",
       "    Velocidade de Aprendizado  Termo Momentum  RMSE Dados Treino  \\\n",
       "0                         1.0             0.1             50.272   \n",
       "1                         1.0             0.5             50.011   \n",
       "2                         1.0             0.1             50.011   \n",
       "3                         1.0             0.5             49.897   \n",
       "4                         0.2             0.1             49.633   \n",
       "5                         1.0             0.1             49.633   \n",
       "6                         0.2             0.5             49.633   \n",
       "7                         1.0             0.1             49.633   \n",
       "8                         0.2             0.5             49.633   \n",
       "9                         0.2             0.1             49.633   \n",
       "10                        0.2             0.5             49.633   \n",
       "11                        0.2             0.1             49.633   \n",
       "12                        1.0             0.5             49.633   \n",
       "13                        0.2             0.5             49.633   \n",
       "14                        0.2             0.1             49.633   \n",
       "15                        1.0             0.5             49.633   \n",
       "16                        1.0             0.5             50.240   \n",
       "17                        1.0             0.1             50.134   \n",
       "18                        0.2             0.1             49.874   \n",
       "19                        1.0             0.1             49.874   \n",
       "20                        0.2             0.5             49.874   \n",
       "21                        0.2             0.1             49.874   \n",
       "22                        1.0             0.1             49.874   \n",
       "23                        0.2             0.5             49.874   \n",
       "24                        0.2             0.1             49.874   \n",
       "25                        1.0             0.5             49.874   \n",
       "26                        0.2             0.5             49.874   \n",
       "27                        0.2             0.1             49.874   \n",
       "28                        1.0             0.5             49.874   \n",
       "29                        1.0             0.1             49.874   \n",
       "30                        0.2             0.5             49.874   \n",
       "31                        1.0             0.5             49.874   \n",
       "\n",
       "    RMSE Dados Teste  \n",
       "0             50.542  \n",
       "1             50.285  \n",
       "2             50.285  \n",
       "3             50.154  \n",
       "4             49.895  \n",
       "5             49.895  \n",
       "6             49.895  \n",
       "7             49.895  \n",
       "8             49.895  \n",
       "9             49.895  \n",
       "10            49.895  \n",
       "11            49.895  \n",
       "12            49.895  \n",
       "13            49.895  \n",
       "14            49.895  \n",
       "15            49.895  \n",
       "16            49.892  \n",
       "17            49.762  \n",
       "18            49.497  \n",
       "19            49.497  \n",
       "20            49.497  \n",
       "21            49.497  \n",
       "22            49.497  \n",
       "23            49.497  \n",
       "24            49.497  \n",
       "25            49.497  \n",
       "26            49.497  \n",
       "27            49.497  \n",
       "28            49.497  \n",
       "29            49.497  \n",
       "30            49.497  \n",
       "31            49.497  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imprime dataframe com valores de RMSE de dados de teste descendentes\n",
    "df_table_reg = df_table_reg.sort_values(by='RMSE Dados Teste', ascending=False, ignore_index=True)\n",
    "df_table_reg.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM DO SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analisando os dados da tabela class além do script disponibilizado, pode-se concluir que, para o caso de classificação, o termo momentum de valor 0,2 foi sistematicamente melhor quando comparado ao valor de 0,5. Quando utilizado o valor 0,2, frequentemente o erro máximo aceitável foi atingido e, eventualmente, não sendo necessários todos os ciclos de treinamento disponibilizados à rede. Além disso pode-se notar que as únicas ocorrências de acurácia maior para termo momentum 0,2 em relação ao termo momentum 0,5 ocorreu com velocidade de aprendizado valor 0,2 e apenas uma camada intermediária, indicando um possível caminho para eventual tunning da rede e nos lembrando que não necessariamente uma rede mais complexa será mais eficiente.\n",
    "- Já analisando os dados da tabela reg além do script disponibilizado, pode-se concluir que, para o caso de regressão multivariada a rede não foi eficiente, apresentando erro quadrado altíssimo em relação à grandeza observada (latitude e longitude). Apesar deste fato, podemos notar que a rede com proporção de dados de treino de 0,5 foi sistematicamente melhor que a proporção 0,8 (i.e. 80% reino e 20% teste) . Este resultado sugere que a rede projetada precisa de mais alternativas para lidar com um problema de regressão. sugere-se, por exemplo, a adoção de ativadores não sigmoidais, como Unidade Linear Retificada (ReLU) e linear.\n",
    "- Observando ambas ocorrências podemos tirar algumas conclusões sobre o caso geral, como: não há uma rede ótima que funciona para qualquer problema de predição, já que seus parâmetros e arquitetura definem sua melhor aplicabilidade; a etapa de tunning de uma rede é especialmente importante, tendo em vista a sensibilidade da mesma aos seus hiperparâmetros.\n",
    "- Sugere-se, para eventual evolução da rede criada, a adição de novos ativadores e mais etapas de tunning sejam realizadas, garantindo a otimização dos hiperparâmetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
